{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h1>SMS Spam Collection Data Set</h1></b>\n",
    "<p><h2>Dataset information</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corpus has been collected from free or free for research sources at the Internet:\n",
    "\n",
    "-> A collection of 425 SMS spam messages was manually extracted from the Grumbletext Web site. This is a UK forum in which cell phone users make public claims about SMS spam messages, most of them without reporting the very spam message received. The identification of the text of spam messages in the claims is a very hard and time-consuming task, and it involved carefully scanning hundreds of web pages. The Grumbletext Web site is: [Web Link].\n",
    "-> A subset of 3,375 SMS randomly chosen ham messages of the NUS SMS Corpus (NSC), which is a dataset of about 10,000 legitimate messages collected for research at the Department of Computer Science at the National University of Singapore. The messages largely originate from Singaporeans and mostly from students attending the University. These messages were collected from volunteers who were made aware that their contributions were going to be made publicly available. The NUS SMS Corpus is avalaible at: [Web Link].\n",
    "-> A list of 450 SMS ham messages collected from Caroline Tag's PhD Thesis available at [Web Link].\n",
    "-> Finally, we have incorporated the SMS Spam Corpus v.0.1 Big. It has 1,002 SMS ham messages and 322 spam messages and it is public available at: [Web Link]. This corpus has been used in the following academic researches:\n",
    "\n",
    "[1] GÃ³mez Hidalgo, J.M., Cajigas Bringas, G., Puertas Sanz, E., Carrero GarcÃ­a, F. Content Based SMS Spam Filtering. Proceedings of the 2006 ACM Symposium on Document Engineering (ACM DOCENG'06), Amsterdam, The Netherlands, 10-13, 2006.\n",
    "\n",
    "[2] Cormack, G. V., GÃ³mez Hidalgo, J. M., and Puertas SÃ¡nz, E. Feature engineering for mobile (SMS) spam filtering. Proceedings of the 30th Annual international ACM Conference on Research and Development in information Retrieval (ACM SIGIR'07), New York, NY, 871-872, 2007.\n",
    "\n",
    "[3] Cormack, G. V., GÃ³mez Hidalgo, J. M., and Puertas SÃ¡nz, E. Spam filtering for short messages. Proceedings of the 16th ACM Conference on Information and Knowledge Management (ACM CIKM'07). Lisbon, Portugal, 313-320, 2007."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h3>STEP-1: Loading Data</h3></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                            message\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('SMSSpamCollection',sep='\\t',names=['target','message'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h3>STEP-2: Performing Exploratory Data Analysis</h3></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   target   5572 non-null   object\n",
      " 1   message  5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "#performing exploratory data analysis\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 7)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       target                 message\n",
       "count    5572                    5572\n",
       "unique      2                    5169\n",
       "top       ham  Sorry, I'll call later\n",
       "freq     4825                      30"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above it can be observed that most the target is of ham category.\n",
    "<p>It can also be observed that there are 5169 unique message out of total 5572 messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.865937\n",
       "spam    0.134063\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we can notice that 13.4% of the given data is **spam**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x24c1fb87ac8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='target',data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h3>STEP-3: Preprocess the Data</h3></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Devendra\n",
      "[nltk_data]     Solanki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Devendra\n",
      "[nltk_data]     Solanki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "# # We can also use Lemmatizer instead of Stemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# initializing the lists\n",
    "\n",
    "clean_lst = []\n",
    "\n",
    "len_lst = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(raw_data, flag):\n",
    "    # Removing special characters and digits\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \",raw_data)\n",
    "    \n",
    "    # change sentence to lower case\n",
    "    letters_only = letters_only.lower()\n",
    "\n",
    "    # tokenize into words\n",
    "    words = letters_only.split()\n",
    "    \n",
    "    # remove stop words                \n",
    "    words = [w for w in words if not w in stopwords.words(\"english\")]\n",
    "    \n",
    "    #Stemming/Lemmatization\n",
    "    if(flag == 'stem'):\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "    else:\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    clean_lst.append(\" \".join(words))\n",
    "    \n",
    "    len_lst.append(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_lst = []\n",
    "\n",
    "len_lst = []\n",
    "\n",
    "df['message'].apply(lambda x: preprocess(x, 'stem'))\n",
    "\n",
    "df['clean_stem'] = clean_lst\n",
    "\n",
    "df['stem_length'] = len_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_lst = []\n",
    "\n",
    "len_lst = []\n",
    "\n",
    "df['message'].apply(lambda x: preprocess(x, 'lemma'))\n",
    "\n",
    "df['clean_lemma'] = clean_lst\n",
    "\n",
    "df['lemma_length'] = len_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>message</th>\n",
       "      <th>clean_stem</th>\n",
       "      <th>stem_length</th>\n",
       "      <th>clean_lemma</th>\n",
       "      <th>lemma_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point crazi avail bugi n great world...</td>\n",
       "      <td>16</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joke wif u oni</td>\n",
       "      <td>6</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entri wkli comp win fa cup final tkt st m...</td>\n",
       "      <td>21</td>\n",
       "      <td>free entry wkly comp win fa cup final tkts st ...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say earli hor u c alreadi say</td>\n",
       "      <td>9</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah think goe usf live around though</td>\n",
       "      <td>7</td>\n",
       "      <td>nah think go usf life around though</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                            message  \\\n",
       "0    ham  Go until jurong point, crazy.. Available only ...   \n",
       "1    ham                      Ok lar... Joking wif u oni...   \n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3    ham  U dun say so early hor... U c already then say...   \n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                          clean_stem  stem_length  \\\n",
       "0  go jurong point crazi avail bugi n great world...           16   \n",
       "1                              ok lar joke wif u oni            6   \n",
       "2  free entri wkli comp win fa cup final tkt st m...           21   \n",
       "3                u dun say earli hor u c alreadi say            9   \n",
       "4               nah think goe usf live around though            7   \n",
       "\n",
       "                                         clean_lemma  lemma_length  \n",
       "0  go jurong point crazy available bugis n great ...            16  \n",
       "1                            ok lar joking wif u oni             6  \n",
       "2  free entry wkly comp win fa cup final tkts st ...            21  \n",
       "3                u dun say early hor u c already say             9  \n",
       "4                nah think go usf life around though             7  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>message</th>\n",
       "      <th>clean_stem</th>\n",
       "      <th>stem_length</th>\n",
       "      <th>clean_lemma</th>\n",
       "      <th>lemma_length</th>\n",
       "      <th>target_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point crazi avail bugi n great world...</td>\n",
       "      <td>16</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joke wif u oni</td>\n",
       "      <td>6</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entri wkli comp win fa cup final tkt st m...</td>\n",
       "      <td>21</td>\n",
       "      <td>free entry wkly comp win fa cup final tkts st ...</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say earli hor u c alreadi say</td>\n",
       "      <td>9</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah think goe usf live around though</td>\n",
       "      <td>7</td>\n",
       "      <td>nah think go usf life around though</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                            message  \\\n",
       "0    ham  Go until jurong point, crazy.. Available only ...   \n",
       "1    ham                      Ok lar... Joking wif u oni...   \n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3    ham  U dun say so early hor... U c already then say...   \n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                          clean_stem  stem_length  \\\n",
       "0  go jurong point crazi avail bugi n great world...           16   \n",
       "1                              ok lar joke wif u oni            6   \n",
       "2  free entri wkli comp win fa cup final tkt st m...           21   \n",
       "3                u dun say earli hor u c alreadi say            9   \n",
       "4               nah think goe usf live around though            7   \n",
       "\n",
       "                                         clean_lemma  lemma_length  target_num  \n",
       "0  go jurong point crazy available bugis n great ...            16           0  \n",
       "1                            ok lar joking wif u oni             6           0  \n",
       "2  free entry wkly comp win fa cup final tkts st ...            21           1  \n",
       "3                u dun say early hor u c already say             9           0  \n",
       "4                nah think go usf life around though             7           0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing the target variable\n",
    "\n",
    "df['target_num']=df['target'].apply(lambda x: 0 if x=='ham' else 1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h3>STEP-4: Data Preparation</h3></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into test and train\n",
    "\n",
    "from sklearn.model_selection  import train_test_split\n",
    "\n",
    "train, test = train_test_split(df,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean=[]\n",
    "for msg in train['clean_stem']:\n",
    "    train_clean.append(msg)\n",
    "\n",
    "test_clean=[]\n",
    "for msg in test['clean_stem']:\n",
    "    test_clean.append(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\")\n",
    "\n",
    "train_features = vectorizer.fit_transform(train_clean)\n",
    "\n",
    "test_features = vectorizer.transform(test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words: 5593\n",
      "Type of train_features: <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Shape of input data (4457, 5593)\n"
     ]
    }
   ],
   "source": [
    "print(\"Total unique words:\", len(vectorizer.vocabulary_))\n",
    "\n",
    "print(\"Type of train_features:\", type(train_features))\n",
    "\n",
    "print(\"Shape of input data\", train_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h3>STEP-5: Model Building and evaluation</h3></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classifiers = [\n",
    "    LogisticRegression(),\n",
    "    DecisionTreeClassifier(),\n",
    "    SVC()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LogisticRegression is 0.9838565022421525\n",
      "[[965   1]\n",
      " [ 17 132]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 33%|████████████████████████████                                                        | 1/3 [00:04<00:08,  4.33s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       966\n",
      "           1       0.99      0.89      0.94       149\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.99      0.94      0.96      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 67%|████████████████████████████████████████████████████████                            | 2/3 [02:10<00:40, 40.91s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of DecisionTreeClassifier is 0.9713004484304932\n",
      "[[954  12]\n",
      " [ 20 129]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       966\n",
      "           1       0.91      0.87      0.89       149\n",
      "\n",
      "    accuracy                           0.97      1115\n",
      "   macro avg       0.95      0.93      0.94      1115\n",
      "weighted avg       0.97      0.97      0.97      1115\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [03:45<00:00, 75.05s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVC is 0.9838565022421525\n",
      "[[966   0]\n",
      " [ 18 131]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       966\n",
      "           1       1.00      0.88      0.94       149\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.99      0.94      0.96      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dense_features = train_features.toarray()\n",
    "\n",
    "dense_test = test_features.toarray()\n",
    "\n",
    "for classifier in tqdm(Classifiers):\n",
    "    fit = classifier.fit(dense_features,train['target_num'])\n",
    "    pred = fit.predict(dense_test)\n",
    "    accuracy = accuracy_score(pred,test['target_num'])\n",
    "    print('Accuracy of '+classifier.__class__.__name__+' is '+str(accuracy))\n",
    "    con_metric=metrics.confusion_matrix(test['target_num'],pred)\n",
    "    print(con_metric)\n",
    "    print(metrics.classification_report(test['target_num'],pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
